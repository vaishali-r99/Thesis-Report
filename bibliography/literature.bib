@article{brunton,
author = {Brunton, Steven L. and Noack, Bernd R. and Koumoutsakos, Petros},
title = {Machine Learning for Fluid Mechanics},
journal = {Annual Review of Fluid Mechanics},
volume = {52},
number = {1},
pages = {477-508},
year = {2020},
doi = {10.1146/annurev-fluid-010719-060214},
URL = {https://doi.org/10.1146/annurev-fluid-010719-060214}}
@article{zhang2015,
author = {Ze Jia Zhang and Karthikeyan Duraisamy},
title = {Machine Learning Methods for Data-Driven Turbulence Modeling},
booktitle = {22nd AIAA Computational Fluid Dynamics Conference},
chapter = {},
pages = {},
doi = {10.2514/6.2015-2460},
URL = {https://arc.aiaa.org/doi/abs/10.2514/6.2015-2460},
eprint = {https://arc.aiaa.org/doi/pdf/10.2514/6.2015-2460}
}
@misc{FreeCAD,
  author = {{FreeCAD Community}},
  title = {FreeCAD 0.19: An Open Source Parametric 3D CAD Modeler},
  year = {2021},
  url = {https://www.freecadweb.org/},
  note = {Accessed: 22.12.2023}
}
@article{batchnorm,
      title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}, 
      author={Sergey Ioffe and Christian Szegedy},
      year={2015},
      eprint={1502.03167},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@book{cv,
  title={The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  edition={2},
  chapter={7},
  pages={241-249},
  year={2009},
  publisher={Springer},
  isbn={978-0-387-84857-0},
  url={https://www.springer.com/gp/book/9780387848570},
  note={Discussion on k-fold cross-validation}
}
@article{lecun1998,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE}
}
@article{sgd,
  title={Large scale online learning},
  author={Bottou, L{\'e}on},
  journal={Advances in neural information processing systems},
  volume={22},
  pages={217--224},
  year={2010}
}
@misc{rmsprop,
  title={Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
  author={Tieleman, Tijmen and Hinton, Geoffrey},
  year={2012},
  howpublished={COURSERA: Neural networks for machine learning},
  note={Available online: \url{https://www.coursera.org/course/neuralnets}}
}
@inproceedings{adagrad,
  title={Adaptive subgradient methods for online learning and stochastic optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  booktitle={Journal of Machine Learning Research},
  volume={12},
  number={Jul},
  pages={2121--2159},
  year={2011}
}
@article{adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@book{rumel,
  title={Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  year={1986},
  publisher={MIT Press}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@article{HeXu2019, title={MgNet: A unified framework of multigrid and convolutional neural network}, volume={62}, DOI={10.1007/s11425-019-9547-2}, number={7}, journal={Science China Mathematics}, author={He, Juncai and Xu, Jinchao}, year={2019}, month={May}, pages={1331–1354}} 
@article{unet,
      title={U-Net: Convolutional Networks for Biomedical Image Segmentation}, 
      author={Olaf Ronneberger and Philipp Fischer and Thomas Brox},
      year={2015},
      eprint={1505.04597},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@InProceedings{glorot,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, Xavier and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/glorot10a.html},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}
@article{he2015,
      title={Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1502.01852},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{ling2016, title={Reynolds averaged turbulence modelling using deep neural networks with embedded invariance}, volume={807}, DOI={10.1017/jfm.2016.615}, journal={Journal of Fluid Mechanics}, author={Ling, Julia and Kurzawski, Andrew and Templeton, Jeremy}, year={2016}, pages={155–166}} 
@article{ogoke2020,
doi = {10.1088/2632-2153/ac1fc9},
url = {https://dx.doi.org/10.1088/2632-2153/ac1fc9},
year = {2021},
month = {sep},
publisher = {IOP Publishing},
volume = {2},
number = {4},
pages = {045020},
author = {Francis Ogoke and Kazem Meidani and Amirreza Hashemi and Amir Barati Farimani},
title = {Graph convolutional networks applied to unstructured flow field data},
journal = {Machine Learning: Science and Technology}}
@article{trask2019,
      title={GMLS-Nets: A framework for learning from unstructured data}, 
      author={Nathaniel Trask and Ravi G. Patel and Ben J. Gross and Paul J. Atzberger},
      year={2019},
      eprint={1909.05371},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{parish2016,
title = {A paradigm for data-driven predictive modeling using field inversion and machine learning},
journal = {Journal of Computational Physics},
volume = {305},
pages = {758-774},
year = {2016},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2015.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0021999115007524},
author = {Eric J. Parish and Karthik Duraisamy}}
@inbook{tracey2013,
author = {Brendan D. Tracey and Karthikeyan Duraisamy and Juan J. Alonso},
title = {A Machine Learning Strategy to Assist Turbulence Model Development},
booktitle = {53rd AIAA Aerospace Sciences Meeting},
chapter = {},
pages = {},
doi = {10.2514/6.2015-1287},
URL = {https://arc.aiaa.org/doi/abs/10.2514/6.2015-1287},
eprint = {https://arc.aiaa.org/doi/pdf/10.2514/6.2015-1287}
}
@article{guastoni2020, title={Convolutional-network models to predict wall-bounded turbulence from wall quantities}, volume={928}, DOI={10.1017/jfm.2021.812}, journal={Journal of Fluid Mechanics}, author={Guastoni, Luca and Güemes, Alejandro and Ianiro, Andrea and Discetti, Stefano and Schlatter, Philipp and Azizpour, Hossein and Vinuesa, Ricardo}, year={2021}} 
@article{milano2002,
title = {Neural Network Modeling for Near Wall Turbulent Flow},
journal = {Journal of Computational Physics},
volume = {182},
number = {1},
pages = {1-26},
year = {2002},
issn = {0021-9991},
doi = {https://doi.org/10.1006/jcph.2002.7146},
url = {https://www.sciencedirect.com/science/article/pii/S0021999102971469},
author = {Michele Milano and Petros Koumoutsakos}}
@article{trandum,
author = {Trancossi, Michele and Dumas, Antonio},
year = {2011},
month = {10},
title = {A.C.H.E.O.N.: Aerial Coanda High Efficiency Orienting-jet Nozzle},
journal = {SAE Technical Papers},
doi = {10.4271/2011-01-2737}
}
@article{kipf,
  title={Semi-Supervised Classification with Graph Convolutional Networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}
@article{pnpp,
      title={PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space}, 
      author={Charles R. Qi and Li Yi and Hao Su and Leonidas J. Guibas},
      year={2017},
      eprint={1706.02413},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{newman,
  title={The deflection of plane jets by adjacent boundaries-Coanda effect},
  author={Newman, BG},
  journal={Boundary layer and flow control},
  year={1961},
  publisher={Pergamon}
}
@article{kara, title={Experimental investigation and numerical verification of Coanda effect on curved surfaces using co-flow thrust vectoring}, journal={International Advanced Researches and Engineering Journal}, volume={5}, pages={72–78}, year={2021}, DOI={10.35860/iarej.758397}, author={Kara, Emre and Erpulat, Hüdai}}

@article{panneer, title={Design and analysis of Coanda effect nozzle with two independent streams}, volume={41}, DOI={10.1080/01430750.2018.1480524}, number={8}, journal={International Journal of Ambient Energy}, author={Panneer, M. and Thiyagu, R.}, year={2018}, month={Jul}, pages={851–860}} 


@article{metalearning,
      title={Meta-Learning for Airflow Simulations with Graph Neural Networks}, 
      author={Wenzhuo Liu and Mouadh Yagoubi and Marc Schoenauer},
      year={2023},
      eprint={2306.10624},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{PIGNN,
title = {PIGNN-CFD: A physics-informed graph neural network for rapid predicting urban wind field defined on unstructured mesh},
journal = {Building and Environment},
volume = {232},
pages = {110056},
year = {2023},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2023.110056},
url = {https://www.sciencedirect.com/science/article/pii/S0360132323000835},
author = {Xuqiang Shao and Zhijian Liu and Siqi Zhang and Zijia Zhao and Chenxing Hu}
}