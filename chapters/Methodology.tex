\chapter{Implementation, results and discussion}
\label{chap:Method}
\section{Data pre-processing/preparation}
\subsection{Dataset generation}
Nozzle simulations are carried out for 120 cases, each with a different set of Inlet 1 and Inlet 2 velocities. The velocity ratio between the two velocities lies in the range of [1,10]. Velocity ratio in our case refers to the ratio of higher velocity to that of lower velocity.
%  such that the velocity ratios between the inlets range from 1 to 9.  (Velocity ratio in our case refers to the ratio of higher velocity to that of lower velocity) 
% The simulation results, i.e; the steady-state fields are logged after 1000 and 30000 time-steps. The \gls{CFD} results at 1000 time-steps are still developing and are unstable whereas those at 30000 steps are observed to be stable solutions. Hence, the simulation results after 30000 steps are taken as the ground truth values or target data, while the input to the surrogate model are the results after 1000 steps. The input and target datasets are then transformed into graph data, normalized, and passed to the network. 
Simulation data are obtained at two intervals: 1,000 and 30,000 time-steps. The results at 1,000 time-steps, which are still developing and unstable, serve as inputs to the surrogate model. In contrast, the results at 30,000 time-steps, which represent stable solutions, are considered the ground truth or target data for model training. These datasets are then transformed into graph data, which encapsulate the spatial relationships and properties of flow fields, and normalized to facilitate efficient learning. The GNN architecture is designed to learn these spatial relationships and predict the stable, steady-state fields from the early, unstable simulation results.
\subsection{Transformation of mesh data to graph data}
Conventional \gls{RANS} solvers require substantial distances from domain boundaries to mitigate adverse effects on solutions around the region of interest. However, this is not required for the deep learning task. Hence, we narrow our attention to a small region just enclosing the nozzle as seen in Figure \ref{clipmesh}. We clip the CFD mesh appropriately and resample the velocity and pressure fields to this mesh with reduced spatial extent. We define the cell-centers on the clipped mesh and assign them as the nodes of the graph. Two adjacent cells in the mesh (cells that share an edge) corresponds to their respective nodes ($v_i$ and $v_j$) on the graph being connected by an edge $e_{ij}$. The graph connectivity is then given by the edge index data structure which comprises two lists - one stores the source node indices and the other has the destination node indices. CFD solvers typically assign pressure, velocity and other fields to each cell of the mesh whereas graphs require node features, i.e; fields defined on each node. Therefore, the cell data (fields) are converted to point data at the cell centers making it suitable for graph representation. The simulation data is then saved in a \verb|hdf5| format. This is then directly used to read $u_{x}$, $u_{y}$, $p$, $c_x$, $c_y$, and $\gamma_{\operatorname{tag}}$. The edge index data structure required for the GNN model is generated by computing adjacent cells and storing their indices in a co-ordinate list (COO) format. 
\begin{figure}[ht]
    \centering
    \includegraphics[width=11cm]{images/Methodology/Clipped.png}
    \caption{Depiction of the area of focus for deep learning - the original CFD mesh (left) is clipped and transformed into the region of interest (right)}
    \label{clipmesh}
\end{figure}
\begin{figure}[ht]
    \centering
    \includegraphics[width=6cm]{images/Methodology/AdjMatrix.png}
    \caption{Visualization of the adjacency matrix representing graph connectivity of the clipped CFD mesh with 7329 nodes.}
    \label{adjmat}
\end{figure}
\subsection{Model inputs and outputs}
After data pre-processing, the simulation mesh is considered as a bidirectional graph $\mathbf{G} = (\mathbf{V}, \mathbf{E})$ where the set of $N$ nodes denoted as $\mathbf{V}$ are linked by the set of edges $\mathbf{E}$ of the mesh. To construct a graph, we need,
\begin{itemize}
\item A feature description, consolidated into an $N \times D$ feature matrix $X$ (where $N$ represents the number of nodes, and $D$ denotes the number of input features).
\item The graph connectivity or relationships within nodes is represented in matrix form as an adjacency matrix, $A$ or as an edge set $E$ of the shape $2 \times P$, where $P$ is the number of pairs of connected nodes in $E$.
\end{itemize}
Let each node have $F_X$ features, and $F_Y$ predictions. The GNN maps the set of node features and edge index matrices to predictions as, 
\begin{equation}
    \mathrm{GNN}: \mathbb{R}^{{N} \times F_{\mathrm{X}}}, \mathbb{W}^{2 \times P} \rightarrow \mathbb{R}^{{N} \times F_{\mathrm{Y}}}
    \end{equation}
We then get a graph-level output $Z$ of the shape ${N} \times F_{\mathrm{Y}}$. \\
The node feature vector $\mathbf{x}_i$ and prediction vector $\mathbf{y}_i$ of interest at each node $v_i$ is given as,
\begin{equation}
    \begin{aligned}
    & \mathbf{x}_i=\left[u_{x, i}, u_{y, i},c_{x, i}, c_{y, i}, \gamma_{\operatorname{tag}, i}\right] \\
    & \mathbf{y}_i=\left[u_{x, i}, u_{y, i}, p_i\right]
    \end{aligned}
\end{equation}
where $u_{x, i}$ and $u_{y, i}$ are the node velocities in X and Y directions, $c_{x, i}$ and $c_{y, i}$ are the spatial co-ordinates of the nodes and $p_i$ is the node pressure. $\gamma_{\operatorname{tag}, i}$ is the node tag that defines which cell the node belongs to: inlet, walls or internal mesh. To summarize, our model has 5 input channels (representing node features) and 3 output channels (denoting node predictions). In addition to these channels, the GNN model also requires an edge index matrix to internally compute the adjacency matrix for the graph. 
\subsection{Data normalization}
Data normalization is performed on both input channels (node features) and output channels (target vectors), carried out in three steps outlined below.
\begin{enumerate}
\item Following common practice, we normalize all the fields of interest with respect to the magnitude of free-stream or reference velocity $u_0$ to make them dimensionless. 
\begin{equation}
    \Tilde{u}=u /\left\|u_0\right\|, \quad \Tilde{p}=p /\left\|u_0\right\|^2
\end{equation}
The latter plays a crucial role as it eliminates the quadratic scaling effect present in the pressure values of the target data, effectively flattening the solution space, thereby simplifying the task for the neural network in subsequent stages.
\item Next, we subtract the mean pressure from the dimensionless pressure values. 
\begin{equation}
\hat{p} = \Tilde{p} - p_{mean} , \quad \text{where } p_{mean} = \sum_i p_i / n
\end{equation}
$n$ is the number of training samples and $p_i$ denotes individual pressure values. Without this step, the pressure targets depict an ill-posed learning objective since the random pressure offsets in the solutions lack correlation with the inputs.
\item As a final step, every channel undergoes normalization to the range of [-1, 1] (or [0,1]). This standardization aims to mitigate errors stemming from finite numerical precision during the training period. We opt for the maximum absolute value of each quantity across the entire training dataset to normalize the data. 
\end{enumerate}
The dataset is split into 3 parts and distributed as Training Data : Validation Data : Test Data in the ratio 80:10:10. 
\section{Graph U-Net}
Here, we introduce the Graph U-Net architecture, a foundational framework for the surrogate models used in this work. We analyze the benefits and shortcomings of this model as well as explain the motivation behind developing a modified GNN.
\begin{figure}[ht]
    \centering
    \includegraphics[width=12cm]{images/Methodology/GraphUNet.png}
    \caption{Detailed view of the Graph U-Net architecture applied to fluid dynamics, showcasing an encoder-decoder structure with skip connections. Each layerâ€™s application is annotated with the resultant dimensions, illustrating the feature reduction or expansion throughout the network. Here, L is the number of hidden layers, N is the number of nodes in the graph, and F$_X$ and F$_Y$ denote the number of input and output features respectively.}
    \label{fig:GraphUnet}
\end{figure}
\subsection{GCNConv layer}
GCNs are NNs operating on graph-structured data that extend the concept of convolutional operations from regular grid-like data, such as images, to irregular and non-Euclidean graph structures, using the spectral graph convolution operation GCNConv. The GCNConv layer aggregates information from neighboring nodes and updates the representations of each node based on this aggregated information, expressed as,
\begin{equation}
h_i^{(l+1)} = \sigma \left(\sum_{j \in \mathcal{N}(i)} \frac{1}{c_{ij}} W^{(l)} h_j^{(l)} + B^{(l)} h_i^{(l)} \right)
\end{equation}
where $W^{(l)}$ and $B^{(l)}$ are the learnable weight and bias matrices for layer $l$, $\sigma$ is the activation function and $c_{ij}$ is a normalization factor. 
In contrast to filters in CNNs, the weight matrix is consistent and shared across all nodes. However, unlike pixels, nodes do not have a fixed number of neighbors. To maintain uniform value ranges across all nodes and facilitate comparability between them, we normalize the outcomes based on the degree of the nodes, i.e; the number of connections of each node. Hence, $c_{ij} = \sqrt{\text{deg(i)}}\sqrt{\text{deg(j)}}$ where, $\text{deg(i)}$ and $\text{deg(j)}$ denotes the degree of the nodes $v_i$ and $v_j$ respectively. 
% Some important aspects of graph convolutions are detailed below. 
% \begin{enumerate}
% \item \textbf{Aggregation of neighbor information:} Each node aggregates the features of its neighbors, weighted by the parameters learned in the weight matrix $W^{(l)}$. This process enables nodes to incorporate information from their local neighborhoods, capturing relational dependencies and structural patterns in the graph. 

% % The normalization factor $c_{ij}$ ensures that the aggregated features are appropriately scaled based on the connectivity of nodes.
% \item \textbf{Weight sharing:}
% The weight matrix $W^{(l)}$ is shared across all nodes, allowing the model to capture common patterns and relationships present in the graph. By sharing parameters, GCNs can effectively learn representations from limited labeled data, facilitating transfer learning and adaptation to new graphs or domains.
% \item \textbf{Hierarchical feature learning:}
% GCNConv layer enables hierarchical feature learning by iteratively aggregating information from neighboring nodes. As information propagates through multiple layers, nodes can capture increasingly abstract and high-level features of the graph. 
% \end{enumerate}
\subsection{Top-k pooling layer}
Top k-Pooling selects the $k$ nodes with the highest scores based on a specified criterion. This operation retains the most important nodes in the graph while discarding less relevant nodes, effectively downsampling the graph. It uses a pooling ratio approach, where $k \in (0, 1]$ such that the graph has $\lfloor kN \rfloor$ nodes after the pooling operation. The decision of which nodes to discard is based on a projection score computed against a learnable vector, $\mathbf{\overrightarrow{p}}$. Fully expressed, computing a pooled graph, $(X^l, A^l)$, from an input graph, $(X, A)$ can be described using the following algorithm: 
\begin{enumerate}
    \item Compute a score for each node in the graph based on node importance or feature relevance as,
    \[
\mathbf{\tilde{y}} = \frac{X \mathbf{\overrightarrow{p}}}{k||\mathbf{\overrightarrow{p}}||_2} \]
    \item Select the top $k$ nodes with the highest scores, \[ \mathbf{\overrightarrow{i}} = \text{top-k}(\mathbf{\tilde{y}}, k) \]
    \item Discard the remaining nodes and their associated edges, resulting in a downsampled graph $G^{\prime} = (V^{\prime}, E^{\prime})$ where $V^{\prime}$ contains only the selected nodes.
    \item Retain the features corresponding to the selected nodes to form the downsampled feature and adjacency matrices, \[
        X^{\prime} = (X \odot \tanh(\mathbf{\tilde{y}}))_{\mathbf{\overrightarrow{i}}}, \quad A^{\prime} = A_{\mathbf{\overrightarrow{i}},\mathbf{\overrightarrow{i}}}
       \]
\end{enumerate}
Here, $||.||_2$ represents the L$_2$ norm, top-k selects the top-k indices, $\odot$ denotes element-wise multiplication, and $\cdot_{\mathbf{\overrightarrow{i}}}$ represents an indexing operation that extracts slices at indices specified by $\mathbf{\overrightarrow{i}}$. 
\subsection{Upsampling}
In the Graph U-Net architecture, unpooling is not a distinct operation like in traditional U-Nets. Instead, skip connections are used to implicitly perform unpooling. During decoding, downsampled features from the encoder are combined with zeros or empty features in the decoder using skip connections. This integration effectively restores spatial details and contextual information from the original input graph, ensuring that important features are retained and allowing for the recovery of detailed graph structures. Therefore, unpooling in Graph U-Net is seamlessly integrated into the skip connection mechanism, facilitating the reconstruction of the original graph resolution during decoding.
\subsection{Results and Discussions}
We usually generate two separate CFD datasets, one for the network input and another for the target dataset. However, in this case where the GNN uses the exact architecture of Graph U-Net, we first investigate the ability of the model to reconstruct the target data when the same is given as input. That is, we are interested in the reconstruction of the target dataset without performing a prediction task. We also perform the prediction task on this Graph U-net architecture, which forms the baseline for all the other architectures proposed in this work. The following settings are applied to both the reconstruction and prediction tasks. We implement a 9-fold cross-validation for the training process. The initial learning rate is set to $0.0005$ and a Step LR scheduler is used to decay the learning rate by a factor of $0.75$ after every 100 epochs. We use the Adam optimizer and train on the \gls{RMSE} loss for 500 epochs. The model's hyperparameters are selected by a hyperparameter tuning process. Table \ref{table:complex} presents the model complexity in terms of total number of trainable parameters. \begin{table}[ht]
    \centering
    \caption{Hyperparameter tuning - Table depicting the number of hidden layers, depth, total number of trainable parameters and the training loss measured with the RMSE criterion for the Baseline architecture.}
    \label{table:complex}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{No. of hidden layers} & \textbf{Depth} & \textbf{Total trainable parameters} & \textbf{Training loss (RMSE)} \\
    \hline
    \multirow{4}{*}{48} & 2 & 7k &  \\
    \cline{2-4}
                        & 3 & 12k &  \\
    \cline{2-4}
                        & 4 & 17k & \\
    \cline{2-4}
                        & 5 & 22k &  0.04272\\
    \hline
    \multirow{4}{*}{64} & 2 & 13k & \\
    \cline{2-4}
                        & 3 & 21k &  \\
    \cline{2-4}
                        & 4 & 30k &  \\
    \cline{2-4}
                        & 5 & 31k &  \\
    \hline
    \multirow{4}{*}{128} & 2 & 51k& \\
    \cline{2-4}
                         & 3 & 84k &  \\
    \cline{2-4}
                         & 4 & 117k &   \\
    \cline{2-4}
                         & 5 & 150k &  \\
    \hline
    \end{tabular}
    
    \end{table}
Among the models with similar performance, we choose the one that is the simplest, i.e; has relatively lesser trainable parameters, without compromising on the accuracy or facing the risk of overfitting. Hence, we execute the training process using a GNN architecture with 48 hidden layers and perform the downsampling and upsampling operations 5 times as the depth equals 5. The key parameters of the model as well as the hyperparameters are listed in Table \ref{table:hp}. 
\begin{table}[ht]
    \centering
    \caption{Model hyperparameters}
    \label{table:hp}
    \begin{tabular}{|l|l|}
    \hline
    \textbf{Hyperparameter}    & \textbf{Value/Description} \\
    \hline
    Number of hidden layers    & 48                           \\
    \hline
    Network depth    & 5                          \\
    \hline
    Pooling ratios             & [0.5,0.5,0.5,0.5,0.5]                       \\
    \hline
    Batch size                 & 4                         \\
    \hline
    Activation function        & ReLU                       \\
    \hline
    Weight initialization    &  Kaiming                        \\
    \hline
    Initial learning rate       & 0.0005                       \\
    \hline
    \end{tabular}
    \end{table}
The training, validation loss are quantified with the \gls{RMSE} heuristic as the average over 9 folds after 500 epochs. 
\subsubsection{Reconstruction of dataset}
As mentioned previously, we prescribe the same dataset (simulation results at 30000 time-steps) as the input and target data to evaluate the effectiveness of reconstruction of the Graph U-Net model. The CFD results, predictions of the GNN model and the absolute difference between these data for a simulation case from the test dataset with 18 m/s and 27 m/s as the inlet 1 and inlet 2 velocities respectively are shown in Figure \ref{blrecon}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=14cm]{images/Methodology/Baselinereconstruct.png}
    \caption{Visualization of velocity fields for a simulation case with Inlet 1 velocity = 18 m/s and Inlet 2 velocity = 27 m/s. Here, (a) represents results from CFD, which is the target data, (b) represents the GNN predictions for the velocity field, and (c) is the absolute difference between the target data and GNN predictions} 
    \label{blrecon}
\end{figure}
We observe that although the GNN predicts the trends of the outflow jet similar to the target data, the scale or the range of velocity magnitude are slightly different for the two cases. Thus, the baseline model is unable to accurately capture the velocity fields for reconstruction. The poor reconstruction can be attributed to the lack of explicit unpooling layers in Graph U-Net, which can lead to inadequate reconstruction of the original graph structure during decoding. 
\subsubsection{Prediction of steady-state solutions}
In this case, we predict the steady-state solutions from the earlier time step. The CFD results, predictions of the GNN model and the absolute difference between these data for four simulation cases from the test dataset are shown in Figure \ref{blpred}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=14cm]{images/Methodology/Baselineprediction.png}
    \caption{Visualization of velocity fields for four simulation case with inlet velocity values prescribed as (Inlet 1, Inlet 2) presented in four columns. Here, the first row represents results the target data, the second row corresponds to the GNN predictions for the velocity field, and the last row is the absolute difference between the target data and GNN predictions} 
    \label{blpred}
\end{figure}
As we can understand from the figure, the predicted direction of the outflow jet as well as its magnitude vary significantly from the expected CFD target data. Thus, the baseline model is not feasible for the prediction task of nozzle flow dynamics. To better comprehend and evaluate the model performance, we estimate the training, validation and test loss of the baseline model for both the tasks in Table \ref{table:perform}. Furthermore, we note down the absolute difference between the input and target data for the test dataset of the prediction task. 
\begin{table}[ht]
    \centering
    \caption{Model evaluation metrics of the Baseline model for the reconstruction and prediction tasks.} 
    \label{table:perform}
    \begin{tabular}{|l|l|l|l|l|}
    \hline
    \textbf{Model} & \textbf{Training Loss} & \textbf{Validation Loss} & \textbf{Test Loss} & \textbf{Abs Difference} \\
    \hline
    Baseline - Reconstruction & 0.04272 & 0.04340 & 0.04562 & - \\
    \hline
    Baseline - Prediction & 0.1459 & 0.1503 & 0.1635 & 0.2626\\
    \hline
    \end{tabular}
    \end{table}
    
Some other limitations to this architecture are,  
\begin{enumerate}
    \item \textbf{Limited Scalability:} Originally designed for small graphs with around 100 nodes, Graph U-Net relies on dense matrix multiplications, which are memory-intensive and not scalable. This leads to memory constraints and slower training times, thus making it impractical for complex, large-scale graph data.
    \item \textbf{Computational Overhead:} Graph U-Net conducts dense adjacency matrix multiplication in the forward pass, resulting in longer training and inference times.
    \item \textbf{Limited Expressiveness:} GCNConv layers may have limited expressiveness in capturing higher-order graph structures and capturing long-range dependencies in the graph. Its optimal depth is found to be 2 or 3 layers \cite{kipf}. Deeper models beyond 7 layers can encounter training difficulties due to increased context size per node and heightened risk of overfitting with a larger number of parameters.
\end{enumerate}
Due to these disadvantages, Graph U-Net may exhibit poor performance in terms of both accuracy and efficiency, particularly for complex geometries or large-scale datasets. Hence, there is a paramount necessity to rely on a modified GNN architectures for our work.
\section{Proposed architecture}
In this section, we discuss the architecture of the proposed GNN surrogate model in detail. We elucidate the novel components in the architecture and the adjustments made on the original Graph U-Net framework. Then, we go on to provide comprehensive details on the hyperparameters and other implementation specifics of the proposed GNN model. Finally, we demonstrate the training process and share the predictions, training and test results obtained for the CFD application. The GNN is developed on the Pytorch deep learning framework using the Pytorch Geometric (PyG) library. Training and testing are performed on V100 GPU compute node using a single GPU (nVidia Tesla V100).\\
There are 3 different surrogate models proposed, each with a slight variation of the Graph U-Net architecture, namely, 
\begin{enumerate}
    \item \textit{Graphknn:} This model uses the k-NN interpolation technique used in PointNet++ \cite{pnpp} for unpooling or upsampling of features instead of relying on skip connections. The downsampled features at different depths (levels of coarsening) are stored so that the upsampled node co-ordinates required for k-NN interpolation can be obtained from $[c_{x}, c_{y}]$ at the downsampled feature of the same depth.
    \item \textit{SAGEknn:} In this surrogate, we replace the GCNConv layers of \textit{BL + knn} with GraphSAGE convolutional layers from \cite{SAGE}. 
    \item \textit{MoNetknn:} In this surrogate, we replace the GCNConv layers of \textit{BL + knn} with GMMConv convolutional layers from \gls{MoNet} \cite{MoNet}. 

    % \item \textit{BL + crsn + knn:} In this surrogate, we first obtain the set of coarsened meshes through incremental decimation of the CFD mesh by running a Python script on Paraview. With the mesh indices and co-ordinates of the low-resolution mesh nodes, we compute the downsampled features for the graph using the sampling operator described in the subsection \ref{SO}. Thus, we implement the pooling operation with the help of sampling operator and perform unpooling operation using k-NN interpolation. 
    % \item \textit{BL + SAGE + crsn + knn:} Similar to the previous model in every detail, the only difference in this model is that it uses GraphSAGE layers instead of the GCNConv layers as convolutions.
    % \item \textit{BL + MoNet + sampl. + knn:} This model is also similar to \textit{BL + crsn + knn}, but it applies MoNet blocks for up convolutions and down convolutions. 
\end{enumerate}
We perform hyperparameter tuning for each of the architectures to arrive at the ideal choice of design parameters. The key aspects of the three architectures along with their hyperparameters are tabulated in Table \ref{prophp}. 
% \subsection{Multi-Resolution Graph Net Architecture}
% \section{Model hyperparameters and training parameters}
\begin{table}[ht]
    \centering
    \caption{Features and hyperparameters comparision of 1. \textit{Graphknn}, 2. \textit{SAGEknn}, and 3. \textit{MoNetknn} architectures.}
    \label{prophp}
    \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{Feature/Hyperparameter}    & \textit{Graphknn} & \textit{SAGEknn}   & \textit{MoNetknn} \\
    \hline
    Number of hidden layers    & 48 & 48 & 48                           \\
    \hline
    Network depth    & 3 & 3 & 3                          \\
    \hline
    Convolution layer             &     GCNConv & SAGEConv & GMMConv                   \\
    \hline
    Pooling operation             &    Top-k &  Top-k &  Top-k                     \\
    \hline
    Unpooling operation         &   k-NN interpolation&   k-NN interpolation&   k-NN interpolation                     \\
    \hline
    Batch size                 & 4 & 4& 4                         \\
    \hline
    Activation function        & ReLU  & ReLU  & ReLU                       \\
    \hline
    Weight initialization    &  Kaiming &  Kaiming &  Kaiming                        \\
    \hline
    Initial learning rate       & 0.0005 & 0.0005 & 0.0005                        \\
    \hline
    \end{tabular}
    \end{table}

Similar to the baseline surrogates, we perform 9-fold cross-validation on the training and validation datasets. The optimizer, scheduler, and the loss metric remain the same as the baseline experiments. Figures \ref{case10}, \ref{case110}, \ref{case31} and \ref{case80} depict the prediction results and the absolute errors obtained from the three surrogates for four different flow conditions. 
% \subsection{Parameter study of graph layers}
% The design choices in the model architecture can impact the overall performance of the model. 

To discern the performance of these models, we compute the \gls{RMSE} loss over the training, validation and test datasets, denoted by Table \ref{t:predloss}.
\begin{table}[ht]
    \centering
    \caption{Model evaluation metrics of  1. \textit{Graphknn}, 2. \textit{SAGEknn}, and 3. \textit{MoNetknn} architectures. } 
    \label{t:predloss}
    \begin{tabular}{|l|l|l|l|l|}
    \hline
    \textbf{Model} & \textbf{Training Loss} & \textbf{Validation Loss} & \textbf{Test Loss} & \textbf{Abs Difference} \\
    \hline
     \textit{Graphknn} & 0.134412& 0.1431491 & 0.143952 &  \\
    \hline
    \textit{SAGEknn}& 0.123704 & 0.138665 & 0.14940 & \\
    \hline
    \textit{MoNetknn} &  & &  &  \\
    \hline
    \end{tabular}
\end{table}

The training and inference times of the three models along with the time taken for CFD simulations are denoted by Table \ref{t:times}. 
\begin{table}[ht]
    \centering
    \caption{Time taken for training and testing of  1. \textit{Graphknn}, 2. \textit{SAGEknn}, 3. \textit{MoNetknn} models and 4. Simulation runtime.} 
    \label{t:times}
    \begin{tabular}{|l|l|l|l|l|}
    \hline
    \textbf{Model} & \textit{Graphknn} & \textit{SAGEknn} & \textit{MoNetknn} & Simulation\\
    \hline
    Training time & 0.134412& 0.1431491 & 0.143952 &  \\
    \hline
    Test time & 0.123704 & 0.138665 & 0.14940 & \\
    \hline
    \end{tabular}
\end{table}
