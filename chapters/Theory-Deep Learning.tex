\chapter{Deep learning primer}
\label{chap:Theory-Deep Learning}
This chapter provides a comprehensive explanation of foundational concepts and methodologies in deep learning, with a particular emphasis on Graph Neural Networks (GNNs). It begins by introducing deep learning and elucidating the basics of neural networks, focusing on their architecture and operational principles. Section \ref{optrain} navigates through the intricacies of training neural networks with topics including feature scaling, weights initialization, batch training, and hyperparameter tuning. Subsequently, it delves into optimization techniques, addressing essential elements such as loss functions, backpropagation, learning rates, and optimizers. Furthermore, this chapter examines model evaluation metrics and explores advanced neural network architectures such as CNNs in Section \ref{cnnse} and GNNs in Section \ref{gnnse}, highlighting their important features and components. 
\section{Introduction to machine learning and deep learning}
Machine learning, a dynamic subset of artificial intelligence, is dedicated to developing algorithms that extract insights and patterns from data. This enables systems to enhance their accuracy and decision-making capabilities without being explicitly programmed for each task. The learning process utilizes statistical models and optimization algorithms to iteratively adjust parameters and improve performance. Machine learning approaches are broadly categorized into supervised learning and unsupervised learning based on learning objectives. \\
\textbf{Supervised learning} involves training a model using labeled data, where each input is paired with a corresponding output. During training, the model learns to map input data to output labels by minimizing the difference between its predictions and the true labels. This approach is commonly used for tasks such as classification and regression. \\
\textbf{Unsupervised learning} involves training a model on unlabeled data, where the algorithm aims to find hidden patterns or structures within the data without explicit guidance. It is often used for tasks like anomaly detection, data exploration, and feature learning, where the data lacks labeled examples or where the underlying structure is unknown. \\
Deep learning is a subset of machine learning that has gained significant attention due to its remarkable performance in various tasks, ranging from image and speech recognition to autonomous driving. It utilizes neural networks consisting of multiple layers of interconnected neurons to learn representations of data through iterative processing of input data to make predictions or decisions. 
% Henceforth, when we mention machine learning or deep learning, it refers to supervised learning in this context, unless otherwise specified.
\section{Fundamentals of neural networks}
\gls{ANN} are computational models inspired by the structure and function of biological neural networks \cite{rumel}. They consist of interconnected nodes organized into layers, typically including an input layer, one or more hidden layers, and an output layer. \\ 
A \textbf{perceptron} or an artificial neuron, is the fundamental building block of ANNs. It takes multiple input signals $\left(x_1, x_2 ...x_n\right)$, each weighted by a connection weight $w_1,w_2,...w_n$, sums them up, and applies an activation function \gls{sigma} to produce an output $y = \sigma \left(\mathbf{w}^T\mathbf{x} + b \right)$, where $b$ is the bias, $\mathbf{w}$ and $\mathbf{x}$ are the weight and input vectors, respectively. Perceptrons are arranged in layers to build complex neural network architectures.\\
\begin{figure}[ht]
    \centering
    \includegraphics[width=10cm]{images/Theory-DL/ActFn.png}
    \caption{Schematic representation of a perceptron - the basic computational unit of artificial neural networks, illustrating input connections, weights, bias, and an activation function.}
    \label{fig:Perceptron}
  \end{figure}
\textbf{Activation functions} are usually non-linear functions to introduce non-linearity within the layers of neural networks, allowing them to learn and represent complex relationships in data. Common activation functions include sigmoid, tanh, and \gls{ReLU} and these are plotted in Figure \ref{fig:ActGraphs}. Without the nonlinear transformation via the activation function, the network would be confined to solving only linear problems. \\
\begin{figure}[ht]
    \centering
    \includegraphics[width=8cm]{images/Theory-DL/ActGraphs.png}
    \caption{Comparative visualization of common activation functions used in neural networks: (a) Sigmoid, (b) Tanh, (c) ReLU, and (d) Leaky ReLU.}
    \label{fig:ActGraphs}
  \end{figure}
\textbf{Weights} in a neural network represent the strength of connections between neurons. They are learned parameters to adjust the influence of input signals on the neuron's output. \textbf{Biases} allow neural networks to model the offset from zero output, influencing the activation of neurons regardless of the input.\\
\textbf{\gls{NN}} consist of interconnected layers of perceptrons that process input data to produce output predictions. They can be represented as directed graphs, where nodes correspond to perceptrons and edges depict connections between them. These connections typically carry weighted signals from one neuron's output to another neuron's input. In particular, we are interested in \textbf{feed-forward neural networks}, in which information flows only in one direction from the input layer through one or more hidden layers to the output layer. Each layer processes the input data independently, and the output of one layer serves as the input to the next layer. The connections between neurons do not form directed cycles, ensuring that the network architecture is acyclic. \gls{MLP} are the simplest feed-forward neural networks, described in Figure \ref{fig:MLP}. 
\begin{figure}[ht]
    \centering
    \includegraphics[width=9cm]{images/Theory-DL/MLP.png}
    \caption{Architecture of a Multi-Layer Perceptron (MLP) showcasing an input layer, multiple hidden layers, and an output layer, demonstrating the flow of information in feed-forward neural networks.}
    \label{fig:MLP}
  \end{figure}
\section{Training of neural networks}\label{optrain}
In this section, we delve into the training practices of neural networks, highlighting the optimization of model parameters to enhance task performance. We particularly focus on the methodologies of data handling like data partitioning, feature scaling, regularization techniques, batch training and weight initialization.
% , critical for achieving predictive accuracy across diverse datasets.
The process of training neural networks is aimed at optimizing a model's parameters to improve its performance on given tasks. We also talk about optimization strategies, which leverage algorithms like backpropagation to calculate gradients and apply updates via optimizers such as SGD or Adam.
\subsection{Data partitioning}Data partitioning is a crucial step in deep learning in which the dataset is divided into separate subsets for training, validation, and testing. The data partitioned into training data is used to train the model, and validation data is used to tune hyperparameters and monitor performance. The test data remains unknown during training and is used to evaluate the final model's generalization performance. The partitioning process ensures that the model's performance is assessed accurately on unseen data and provides independent datasets for training and evaluation. 
\subsection{Feature scaling}Feature scaling or normalization, is a preprocessing step aimed at bringing all input features to a similar scale. Features with large magnitudes can lead to large gradients during training, which may cause unstable behavior. Feature scaling mitigates this issue by reducing the range of feature values, thus preventing gradient instability and ensuring more reliable optimization. Common feature scaling techniques include min-max normalization, Z-score standardization, and unit length scaling.
% \begin{enumerate}
% \item Min-Max normalization: $x^{\prime}=\frac{x-x_{\min }}{x_{\max }-x_{\min }}$
% \item Z-Score standardization: $x^{\prime}=\frac{x-\tilde{x}}{\sigma}$
% \item Unit length scaling: $\mathrm{x}^{\prime}=\frac{\mathrm{x}}{\|\mathrm{x}\|}$
% \end{enumerate}
\subsection{Weight initialization}
Weight initialization sets the initial values for the model's parameters before training begins. The initial weights dictate the local minimum the weights should converge to; thus, better initialization leads to improved model performance. % There are different cases to consider for weight initialization: 
% \begin{enumerate}
% \item Initializing all weights to 0 or a constant leads to symmetrical gradients and weight updates across all neurons in the network during backpropagation. This results in the neurons learning identical features, causing the network to lose its representational capacity.
% \item Large weights can lead to exploding gradients during training. Large weights can saturate activation functions, pushing them into regions of zero gradients (e.g., in sigmoid or tanh activations), hindering learning and resulting in slow convergence.
% \item Small weights help prevent exploding gradients, as activations and gradients remain within a manageable range during training. However, extremely small weights results in small activations, leading to vanishing gradients. 
% \end{enumerate}
% In summary, weight initialization is crucial for initializing neural networks effectively. 
Random initialization with small weights is a common practice in deep learning. Initializing weights to random values drawn from a suitable distribution with a zero mean and small variance breaks symmetry and helps prevent both vanishing and exploding gradients. It encourages each neuron to learn different features from the input data, promoting diverse representations and effective learning. Techniques like Xavier \cite{glorot} and Kaiming \cite{he2015} initialization refine this process by adapting to the characteristics of activation functions.
% \subsubsection{Xavier-Glorot initialization}
% This technique initializes weights from a normal or uniform distribution with a zero mean. The variance of the distribution is adjusted based on the number of input neurons (fan-in) and output neurons (fan-out) as $\text{Var}(W) = \frac{2}{\text{fan}_{\text{in}} + \text{fan}_{\text{out}}}$. Xavier initialization is commonly used in shallow networks with symmetric activation functions, ensuring balanced weight initialization and stable training dynamics. 
% \subsubsection{Kaiming-He initialization}
% Kaiming initialization is designed for networks using non-linear activations like ReLU. It initializes weights from a normal distribution with zero mean, adjusting the variance based on fan-in as $ \text{Var}(W) = \frac{2}{\text{fan}_{\text{in}}}$. This method helps mitigate the issue of vanishing gradients associated with ReLU activations, ensuring stable training and faster convergence.
\subsection{Regularization}
Regularization broadly refers to techniques used to prevent overfitting by imposing additional constraints on the model's parameters, i.e.; by adding penalties to the loss function. Regularization penalizes large weights in the model, thereby promoting simpler models that generalize better to unseen data. 
% Two common forms of regularization are L$_1$ regularization (Lasso) and L$_2$ regularization (Ridge). \\
L$_1$ regularization encourages sparsity in the weights, performing feature selection by setting irrelevant weights to zero, making the model simpler and more interpretable. 
\[ \operatorname{Loss}_{\mathrm{L}_1 \text{Reg}}=\operatorname{Loss}_{\text {original }}+\lambda \sum_{i=1}^n\left|w_i\right| \]
L$_2$ regularization encourages the weights to be spread out more evenly, preventing individual weights from becoming too large.
\[ \operatorname{Loss}_{\mathrm{L}_2 \text{Reg}}=\operatorname{Loss}_{\text {original }}+\lambda \sum_{i=1}^n w^2_i \]
Here, \gls{lambda} is the regularization strength and determines the degree of penalty imposed on large weights. A smaller $\lambda$ value results in weaker regularization, increasing the risk of overfitting. Conversely, a larger $\lambda$ value results in a simpler model that generalizes better but may underfit the training data.
% \subsubsection{Dropout}
% Dropout is a regularization technique, where a random fraction of neurons is dropped out or ignored during forward and backward propagation. This prevents neurons from co-adapting and overfitting to the training data, promoting robustness and generalization. 
\subsection{Batch training and batch normalization}
Batch training is a technique in deep learning in which the model updates its parameters based on a subset (or batch) of the training data, rather than the entire dataset. The training data is divided into batches of fixed size, and the model computes the gradient updates based on an average of the samples in each batch. This reduces the variance in the gradient estimates, stabilizes the gradients, and prevents large fluctuations during training. \\
Too small a batch size leads to frequent and noisy gradient updates. On the other hand, too large a batch requires more computational resources and memory, despite providing precise gradient estimates and stable optimization. Selecting an optimal batch size requires balancing the trade-off between computational efficiency and the quality of gradients. \\                     
Another important term in this context is an epoch, which refers to a single pass through the entire training dataset. During one epoch, each batch is processed sequentially through the neural network, completing a full iteration over the entire dataset. Typically, training iterates over the entire dataset for multiple epochs until the model converges or a predefined stopping criterion is met. \\
% Batch normalization (BatchNorm) is another technique used in deep learning to stabilize and accelerate training by normalizing the activations of each layer within a mini-batch. 
% Batch normalization \cite{batchnorm} is a technique to improve the stability of neural networks by normalizing the activations of each layer. It operates on mini-batches of data and normalizes the input of each layer to have a mean of zero and a standard deviation of one. The batch normalization transformation for a layer with input $x^{(1)}, x^{(2)}, \ldots, x^{(m)}$ is given as,

% %  This is achieved by computing the mean and standard deviation of the activations across the batch, and then scaling and shifting the activations using learned parameters, given by, 

% \begin{equation}
% \begin{aligned}
% & \mu_B=\frac{1}{m} \sum_{i=1}^m x^{(i)} \\
% & \sigma_B^2=\frac{1}{m} \sum_{i=1}^m\left(x^{(i)}-\mu_B\right)^2 \\
% & \hat{x}^{(i)}=\frac{x^{(i)}-\mu_B}{\sqrt{\sigma_B^2+\epsilon}} \\
% & y^{(i)}=\gamma \hat{x}^{(i)}+\beta
% \end{aligned}
% \end{equation}
% where $\mu_B$ and $\sigma_B^2$ are the mean and variance of the mini-batch $B$ of size $m$, $\epsilon$ is a small constant added to avoid division by zero, $\hat{x}^{(i)}$ is the normalized input, $\gamma$ and $\beta$ are learnable parameters (scale and shift), and $y^{(i)}$ is the output of the batch normalization layer.
\subsection{Overfitting and underfitting}
Overfitting and underfitting are two common phenomena that affect the performance and generalization ability of the model. Overfitting occurs when a model learns to perform well on the training data but fails to generalize to unseen data. The model becomes overly complex and specific to the training set, leading to poor generalization. Signs of overfitting include high training accuracy but low validation or test accuracy as the model memorizes training examples. Techniques such as regularization, dropout and early stopping can help prevent overfitting by reducing the model's capacity and complexity. \\
% This happens when the model captures noise or random fluctuations in the training data as if they were genuine patterns.
Underfitting occurs when a model is too simple to capture the underlying structure of the data. In this case, the model fails to learn the patterns present in the training data and performs poorly both on the training and unseen data. Underfitting often occurs when the model is too shallow or simple. Signs of underfitting include low training and validation accuracy. Increasing the model's capacity, adding more data, or improving feature engineering can help alleviate underfitting by allowing the model to capture more complex relationships in the data.
\subsection{Hyperparameters}\label{section:hyperparameters}
Hyperparameters in deep learning are fixed parameters set prior to the training process that are not learned from the data. They control various aspects of the learning process, such as the model architecture, optimization settings and the training procedure itself. Some important hyperparameters are the number of neurons per layer, number of layers, activation function, batch size, number of epochs, optimizer, loss function, weight initialization, dropout rate, and regularization strength. \\
Hyperparameter tuning is the process of selecting the optimal values for these hyperparameters to maximize the performance of the model on unseen data. It involves systematically searching through a predefined space of hyperparameters and evaluating the model's performance using a validation set or cross-validation. The goal is to find the hyperparameters that result in the best generalization performance, balancing between underfitting and overfitting.
\subsubsection{k-Fold cross-validation}
In k-fold cross-validation \cite{cv}, the dataset is divided into $k$ subsets or folds, of approximately equal size. The model is trained k times, each time using k-1 folds for training and the remaining fold for validation. This process is repeated k times, with each fold used exactly once as the validation set.  Instead of relying on a single validation set, this method averages the performance over multiple folds, providing a more stable estimate of the model's performance. k-fold cross-validation helps evaluate the performance of different hyperparameter configurations during hyperparameter tuning. Figure \ref{fig:crossval} represents \gls{LOOCV} - a variant of k-fold cross-validation. 
\begin{figure}[ht]
  \centering
  \includegraphics[width=10cm]{images/Theory-DL/crossval.png}
  \caption{Schematic Representation of LOOCV - In every iteration, exactly one fold is subsequently chosen as the validation fold while the remaining (k-1) folds are combined to become the training dataset.}
  \label{fig:crossval}
\end{figure}
\subsection{Optimization} 
Optimization involves adjusting the parameters of the neural network, such as weights and biases, to minimize a predefined objective function, typically referred to as the loss function. The optimization process iteratively updates the parameters based on the gradients of the loss function with respect to the network's parameters, aiming to converge to a set of optimal parameters that yield the best performance on the given task. Some important aspects of the optimization process are discussed in the following subsections. 
\subsubsection{Loss function}
The loss function quantifies the difference between the model's predictions and the actual target values. It represents the measure of how well the model is performing on the training data. Common loss functions include mean squared error (MSE) for regression tasks and categorical cross-entropy for classification tasks. The loss function \gls{l} is defined as: 
\begin{equation}
    \mathcal{L}(\theta)=\frac{1}{N} \sum_{i=1}^N L\left(y_i, \hat{y}_i ; \theta\right)
    \end{equation}
Here, \gls{theta} represents the parameters of the neural network being optimized, such as weights and biases, \gls{yi} is the ground truth and \gls{yihat} is the model prediction. The loss function $\mathcal{L}(\theta)$ depends on these parameters, and it is computed as the average of the individual loss $L\left(y_i, \hat{y}_i ; \theta\right)$ over all the training examples of size $N$.
\subsubsection{Backpropagation}
Backpropagation is a fundamental algorithm used to compute the gradients of the loss function with respect to the parameters (weights and biases) of the neural network. It involves propagating the error backward from the output layer to the input layer, updating the parameters along the way to minimize the loss. The gradients are computed using the chain rule of calculus, enabling efficient optimization of the network's parameters. The gradients $\nabla_\theta \mathcal{L}(\theta)$ of the loss function are computed as,
\begin{equation}
    \nabla_\theta \mathcal{L}(\theta)=\frac{1}{N} \sum_{i=1}^N \nabla_\theta L\left(y_i, \hat{y}_i ; \theta\right)
    \end{equation}
\subsubsection{Learning rate}
The learning rate is a hyperparameter that controls the size of the parameter updates, that is, the step-size in the direction of the gradients computed by backpropagation. A higher learning rate may lead to faster convergence but risks overshooting the optimal solution, while a lower learning rate may result in slower convergence but more stable training. The parameter update rule with learning rate \gls{eta} is given by:
\begin{equation}
    \theta_{t+1}=\theta_t-\eta \nabla_\theta \mathcal{L}(\theta)
    \end{equation}
Here, $\theta_{t+1}$ and $\theta_t$ represent the parameters at time-step $t$ and $t+1$ respectively. Learning rate decay is often used to gradually reduce the learning rate during training with the help of learning rate schedulers.
% \begin{figure}[ht]
%   \centering
%   \includegraphics[width=10cm]{images/Theory-DL/LR.png}
%   \caption{Effect of Learning Rates}
%   \label{fig:LR}
%   \end{figure}
\subsubsection{Optimizer} 
The optimizer is responsible for updating the parameters of the neural network based on the gradients computed during backpropagation. It determines the direction and magnitude of parameter updates to minimize the loss function efficiently. Popular optimizers include \gls{SGD} \cite{sgd}, \gls{Adam} \cite{adam}, \gls{RMSProp} \cite{rmsprop}, and \gls{AdaGrad} \cite{adagrad}. We use the Adam optimizer in the training and testing phases of our work. \\
 \textbf{Adam (Adaptive Moment Estimation)} is an algorithm for stochastic optimization that combines the ideas of SGD with momentum and RMSProp. It maintains exponentially decaying moving averages of past gradients and past squared gradients for each parameter. These moving averages serve as estimates of the first and second moments of the gradients, respectively. Adam also incorporates bias correction terms to compensate for the initial bias towards zero at the beginning of training. The parameter update rules at time-step $t$ are given by,
 \begin{enumerate}
  \item Compute the gradient of the loss function with respect to the parameters, \(\theta_t\).
  \item Update biased first moment estimate: \(m_t = \beta_1 m_{t-1} + (1 - \beta_1)\theta_t\).
  \item Update biased second raw moment estimate: \(v_t = \beta_2 v_{t-1} + (1 - \beta_2)\theta_t^2\).
  \item Compute bias-corrected first moment estimate: \(\hat{m}_t = \frac{m_t}{(1 - \beta_1^t)}\).
  \item Compute bias-corrected second moment estimate: \(\hat{v}_t = \frac{v_t}{(1 - \beta_2^t)}\).
  \item Update the parameters: \(\theta_{t+1} = \theta_t - \eta \cdot \frac{\hat{m}_t}{(\sqrt{\hat{v}_t} + \epsilon)}\).
\end{enumerate}
Here, \(\beta_1\) and \(\beta_2\) are exponential decay rates for these moment estimates, typically close to 1, and $\epsilon$ is a small constant added to avoid division by zero.
\section{Model evaluation metrics}
Model evaluation metrics are essential for assessing the performance of deep learning models. The loss function serves as a crucial metric for evaluating model performance, in addition to optimizing model parameters during training. In regression tasks, where the goal is to predict continuous values, common loss functions include the following:
\begin{itemize}
\item \textbf{\gls{MAE}:} MAE measures the average absolute difference between the predicted values and the actual values:\[ \text{MAE} = \frac{1}{N} \sum_{i=1}^{N} |y_i - \hat{y}_i| \]  MAE is robust to outliers and does not penalize large errors heavily.
% \textbf{Mean Relative Error (MRE)}
\item \textbf{\gls{MSE}:} MSE measures the average squared difference between the predicted values and the actual values:\[ \text{MSE} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2 \] MSE penalizes larger errors more heavily than MAE since errors are squared. This makes it more sensitive to outliers. 
\item \textbf{\gls{RMSE}:} RMSE is the square root of the average squared difference between the predicted values and the actual values: \[ \text{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2} \]RMSE is sensitive to outliers, similar to MSE, but is more interpretable as it is in the same units as the target variable.
\end{itemize}
\section{Convolutional Neural Networks (CNNs)} \label{cnnse}
CNNs \cite{lecun1998} are specialized neural networks designed to process grid-like data, such as images. They utilize convolutional layers and pooling operators to learn spatial hierarchies of features from input data, making them highly effective for tasks like image classification, object detection, and image segmentation. In this section, we outline the major components of CNNs, such as convolutional layers, pooling and unpooling operations. 
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=10cm]{images/Theory-DL/CNN.png}
%     \caption{Diagram illustrating the typical structure of a CNN consisting of convolutional, pooling, and fully connected layers. Image taken from \cite{cnnimage}.}
%     \label{fig:CNN}
%   \end{figure}
\subsection{Convolutional layer}
The convolutional layer in a CNN consists of a set of learnable filters (kernels) that slide over the input data, performing element-wise multiplication and summing to produce feature maps. The convolution operation preserves the spatial relationship between pixels and learns local patterns like edges, textures, and shapes. Figure \ref{fig:Conv1} shows the working of a convolution kernel. 
\begin{figure}[ht]
    \centering
    \includegraphics[width=14cm]{images/Theory-DL/Conv1.png}
    \caption{Detailed view of a convolutional layer's operation in a CNN, depicting the convolution process over an input matrix with a specified kernel to produce feature maps.}
    \label{fig:Conv1}
  \end{figure}
\subsection{Pooling and unpooling} Pooling and unpooling operators perform sampling operations, enabling hierarchical feature extraction while preserving spatial information. Pooling is a down-sampling operation commonly used in CNNs to reduce the spatial dimensions of feature maps. Max pooling and average pooling are popular pooling techniques that select the maximum or average value within each pooling region, respectively. These operations are depicted in Figure \ref{fig:Pool}. Conversely, unpooling layers, used in upsampling, reconstruct the original input resolution from the lower-dimensional representations generated by pooling. These layers store the indices of the maximum values during pooling and use them for upsampling. Nearest neighbor interpolation is a simple upsampling method where each pixel in the input is replicated multiple times to form the output, as seen in Figure \ref{fig:Unpool2}.
\begin{figure}[ht]
    \centering
    \includegraphics[width=6cm]{images/Theory-DL/Pool.png}
    \caption{Visualization of pooling operations in CNNs, (a) max pooling, and (b) average pooling.}
    \label{fig:Pool}
\end{figure}
\begin{figure}[ht]
        \centering
        \includegraphics[width=6cm]{images/Theory-DL/NNUnpool.png}
        \caption{Illustration of nearest neighbor interpolation based unpooling used in CNNs.}
        \label{fig:Unpool2}
    \end{figure}
\subsection{The U-Net architecture}
U-Net \cite{unet} is a CNN consisting of a U-shaped network structure with a contracting path (encoder) followed by an expanding path (decoder), which is widely used for image segmentation. Some important components of the U-Net architecture are discussed below. 
\begin{figure}[ht]
    \centering
    \includegraphics[width=12cm]{images/Theory-DL/UNet.png}
    \caption{Structure of the U-Net architecture demonstrating its U-shaped design with contracting and expanding paths. Image taken from \cite{unet}.}
    \label{fig:UNet}
\end{figure}
\begin{enumerate}
  \item The \textbf{encoder} comprises a series of down-convolutional and max pooling layers that gradually reduce the spatial dimensions of the input image while increasing the number of feature channels. This path extracts high-level features from the input image while preserving spatial context.
  \item The \textbf{decoder} consists of up-convolutional (transposed convolution) and concatenation layers that gradually increase the spatial dimensions of the feature maps while reducing the number of feature channels. This path generates segmentation masks by upsampling the low-resolution feature maps obtained from the encoder and combining them with high-resolution feature maps using skip connections.
  \item \textbf{Skip connections} or residual connections \cite{he2016deep}, are direct connections between layers at the same hierarchical level in the network. Skip connections connect the encoder to the corresponding layers in the decoder. This enables the network to retain fine-grained spatial information from the encoder while recovering spatial details lost during downsampling. 
\end{enumerate}
\begin{figure}[ht]
    \centering
    \includegraphics[width=5cm]{images/Theory-DL/Skip.png}
    \caption{Diagram demonstrating the concept of skip connections within neural network architectures. Skip connections bypass one or more layers by directly feeding the output from an earlier layer to a later layer.} 
    \label{fig:Skip}
\end{figure}
A notable observation pertinent to our current endeavor is the striking resemblance between the U-Net architecture and the V-cycle multi-grid method, as noted by He and Xu \cite{HeXu2019}. Both employ a hierarchical structure wherein information is exchanged across varying resolutions.
A major limitation of CNNs is their inability to directly operate on irregular data formats, such as social networks, recommender systems, molecular structures, or citation networks. In contrast to CNNs, which are well-suited for grid-like structured data such as images, GNNs are tailored for data represented as graphs, which are discussed in the upcoming section.
\section{Graph Neural Networks (GNNs)} \label{gnnse}
In 2017, Kipf and Welling introduced the Graph Convolutional Network (GCN) \cite{kipf}, a foundational architecture that laid the groundwork for modern Graph Neural Networks (GNNs). Since then, numerous advancements and variants of GNNs have been proposed. Graph data are characterized by non-Euclidean and irregular structures, where entities (nodes) and their relationships (edges) vary in connectivity and structure. GNNs excel in processing unstructured data by leveraging the inherent graph structure by dynamically aggregating information from neighboring nodes based on their connectivity. Formally, a graph \gls{G} can be denoted as \( G = (V, E) \), where \gls{V} is the set of nodes and \gls{E} is the edge set. The key components of a graph are outlined below.
\begin{enumerate}
  \item \textbf{Nodes}: Nodes represent entities in a graph, such as users in a social network, atoms in a molecule, or words in a document. 
  \item \textbf{Edges}: Edges define relationships or connections between nodes in a graph. Each edge \( e_{ij} \) connects node \( v_i \) to node \( v_j \), where \( v_i, v_j \in V \). The edge set $E$ can be represented as a collection of tuples \( (v_i, v_j) \) indicating the connections between nodes.
  \item \textbf{Adjacency matrix}: An adjacency matrix is a binary $N \times N$ matrix representing the connections between $N$ nodes in a graph. For an undirected graph, \( A_{ij} \) is 1 if there exists an edge between nodes \( v_i \) and \( v_j \), and 0 otherwise. For directed graphs, the adjacency matrix may be asymmetric to represent the directionality of edges. The adjacency matrix \gls{A} of the graph \( G \) can be defined as,
  \[
  A_{ij} = \begin{cases} 1 & \text{if } (v_i, v_j) \in E \\ 0 & \text{otherwise} \end{cases}
  \]
  \item \textbf{Node attributes and node feature matrix}: Node attributes or features represent information associated with each node in the graph. These features can encode characteristics such as velocity, pressure, and temperature as node embeddings. The node feature matrix \gls{X} for a graph with \( N \) nodes and \( D \) features is a $N \times D$ matrix where each row corresponds to a node and each column represents a feature dimension, given by,
\begin{equation*}
  X = \begin{bmatrix} x_1^T \\ x_2^T \\ \vdots \\ x_N^T \end{bmatrix} \quad \text{ where,} \quad x_i = \begin{bmatrix} x_{i1} \\ x_{i2} \\ \vdots \\ x_{iD} \end{bmatrix}
\end{equation*}
where \( x_i \) represents the feature vector associated with node \(v_i \).
  \item \textbf{Edge weights}: Edge weights \gls{W} quantify the strength or intensity of relationships between nodes connected by edges. These weights can represent similarity measures, distances, or any other relevant information associated with edge connections. Similar to node weights, edge weights can be learned or predefined. 
\end{enumerate}  
\begin{figure}[ht]
  \centering
  \includegraphics[width=12cm]{images/Theory-DL/AdjMat.png}
  \caption{Schematic diagram of graph connectivity depicted by an adjacency matrix using binary representation of node relationships in graph-structured data.}
  \label{fig:AdjMat}
\end{figure}
GNNs leverage these components to perform message passing and aggregation operations across the graph structure, which are discussed below. \\
\subsection{Graph convolutions}
Graph convolutions update the feature representations of nodes in a graph by aggregating information from their neighboring nodes. Graph convolutional operators typically exhibit local connectivity, where each node's representation is updated based on the information from its neighboring nodes. This local connectivity property allows the model to capture localized patterns and dependencies within the graph structure. Weight sharing is a key aspect of graph convolutions, where the same set of learnable parameters (weights) is shared across different nodes in the graph. This allows for parameter efficiency and enables the model to generalize well to unseen nodes and graphs. GCNConv \cite{kipf}, GMMConv \cite{MoNet}, and SAGEConv \cite{SAGE} are some common classes of convolution operators. Here, we discuss the GCNConv operator that is used in Graph Convolutional Networks (GCNs).
% There are two ways to perform graph convolution operations. In spectral convolution, graph signals are transformed into the spectral domain using techniques like graph Fourier transforms. Spatial convolution directly operates on the graph's topology and neighborhood structure, aggregating information from neighboring nodes without explicitly transforming the graph. In this work, we only deal with spatial convolutions and refer to them as graph convolutions henceforth. 
\subsubsection{GCNConv}
GCNConv aggregates information from neighboring nodes and updates the representations of each node based on this aggregated information. The main steps involved are:
\begin{enumerate} 
    \item \textbf{Message passing}: Nodes exchange messages with their neighbors, aggregating information from neighboring nodes. The message passed from node $v_j$ to node $v_i$ at layer $l$ can be represented as:
        \[ m_{ij}^{(l)} = \frac{1}{c_{ij}} \mathbf{W}^{(l)} h_j^{(l)} + \mathbf{B}^{(l)} h_i^{(l)}  \]
        where $m_{ij}^{(l)}$ is the message from node $j$ to node $i$, \gls{Wl} and \gls{Bl} are the learnable weight and bias matrices, \gls{hil} denotes the feature vector of node $i$ at layer $l$, and $c_{ij}$ is a normalization factor.
    \item \textbf{Aggregation}:Nodes aggregate the messages received from their neighbors to update their own feature representations. The aggregated message $a_i^{(l)}$ for node $i$ can be computed as the sum or average of the incoming messages.
        \[ a_i^{(l)} = \sum_{j \in \mathcal{N}(i)} m_{ij}^{(l)} \]
    where \gls{ni} denotes the set of neighboring nodes of $v_i$.
    \item \textbf{Update}: Nodes update their feature representations using the aggregated messages and their own features. The updated feature representation $h_i^{(l+1)}$ for node $i$ at layer $l+1$ can be computed as:
        \[ h_i^{(l+1)} = \sigma(a_i^{(l)}) = \sigma \left(\sum_{j \in \mathcal{N}(i)} \frac{1}{c_{ij}} \mathbf{W}^{(l)} h_j^{(l)} + \mathbf{B}^{(l)} h_i^{(l)} \right)\]
\end{enumerate}
These steps are performed iteratively across multiple layers of the GNN. At each layer, nodes update their feature representations based on the aggregated messages. The iterative propagation of messages allows nodes to incorporate information from distant parts of the graph and refine their representations over multiple layers. \\
% In contrast to filters in CNNs, the weight matrix is consistent and shared across all nodes. However, unlike pixels, nodes do not have a fixed number of neighbors. To maintain uniform value ranges across all nodes and facilitate comparability between them, we normalize the outcomes based on the degree of the nodes, i.e; the number of connections of each node. Hence, $c_{ij} = \sqrt{deg(i)}\sqrt{deg(j)}$ where, ${deg(i)}$ and ${deg(j)}$ denotes the degree of the nodes $v_i$ and $v_j$ respectively. 
% \subsubsection{SAGEConv (GraphSAGE Convolution)}
% The SAGEConv operator stems from the GraphSAGE (Graph Sample and AggregatE) framework that updates the feature vector $x_i$ of a node $i$ by aggregating features from its immediate neighbors $\mathcal{N}(i)$ and combining this aggregated information with the node's own features. The updated feature vector is calculated using the following formula:
% \[
% \mathbf{x}_i' = \mathbf{W}_1 \mathbf{x}_i + \mathbf{W}_2 \cdot \text{mean}_{j \in N(i)} \mathbf{x}_j
% \]
% where  $\mathbf{W}_1$ and $\mathbf{W}_2$ are trainable weight matrices and $\text{mean}_{j \in N(i)} \mathbf{x}_j$ is the mean aggregation function that computes the element-wise mean of the feature vectors of the neighboring nodes $j \in N(i)$. 
% \subsubsection{GMMConv (Gaussian Mixture Model Convolution)}
% GMMConv models the neighborhood aggregation process using a Gaussian Mixture Model (GMM), which updates the feature \( x_i \) of each node \( i \) using the features of its neighboring nodes, given by, 
% \[
% \mathbf{x}_i^{\prime}=\frac{1}{|\mathcal{N}(i)|} \sum_{j \in \mathcal{N}(i)} \frac{1}{K} \sum_{k=1}^K \mathbf{w}_k\left(\mathbf{e}_{i, j}\right) \odot \boldsymbol{\Theta}_k \mathbf{x}_j\]
%    where, $|\mathcal{N}(i)|$ is a normalization constant based on the neighborhood of node $i$ and \( \mathbf{w}_k(\mathbf{e}_{ij})\) is the weighting function under the \( k \)-th Gaussian component for edge \( e_{i,j} \) between nodes \( i \) and \( j \) given by,
%    \[
%    \mathbf{w}_k(\mathbf{e})=\exp \left(-\frac{1}{2}\left(\mathbf{e}-\mu_k\right)^{\top} \Sigma_k^{-1}\left(\mathbf{e}-\mu_k\right)\right)
%    \]
% Here, the mean vector \( \mu_k \), and the diagonal covariance matrix \( \Sigma_k \) are trainable parameters.
\subsection{Graph pooling}
Graph pooling aggregates node representations across the entire graph to compute global graph-level features and create a coarser graph representation. It reduces the size of the graph representation while preserving important structural and relational information. The different types of graph pooling are:
\subsubsection{Top-k pooling} 
This algorithm selects the top k nodes based on criteria such as node importance or feature values, and aggregates their information to create a coarser graph representation. It retains the most informative nodes while reducing the size of the graph, making it suitable for tasks requiring node selection or summarization. It uses a pooling ratio approach such that the graph has $\lfloor kN \rfloor$ nodes after the pooling operation, where $k \in (0, 1]$ is the pooling ratio. The decision of which nodes to discard is based on a projection score computed against a learnable vector. 
% , $\mathbf{\overrightarrow{p}}$. Fully expressed, computing a pooled graph, $(X^l, A^l)$, from an input graph, $(X, A)$ can be described using the following steps: 
% \begin{enumerate}
%     \item Compute a score for each node in the graph based on node importance or feature relevance as,
%     \[
% \mathbf{\tilde{y}} = \frac{X \mathbf{\overrightarrow{p}}}{k||\mathbf{\overrightarrow{p}}||_2} \]
%     \item Select the top $k$ nodes with the highest scores, \[ \mathbf{\overrightarrow{i}} = \text{top-k}(\mathbf{\tilde{y}}, k) \]
%     Discard the remaining nodes and their associated edges, resulting in a downsampled graph $G^{\prime} = (V^{\prime}, E^{\prime})$ where $V^{\prime}$ contains only the selected nodes.
%     \item Retain the features corresponding to the selected nodes to form the downsampled feature and adjacency matrices, \[
%         X^{\prime} = (X \odot \tanh(\mathbf{\tilde{y}}))_{\mathbf{\overrightarrow{i}}}, \quad A^{\prime} = A_{\mathbf{\overrightarrow{i}},\mathbf{\overrightarrow{i}}}
%        \]
% \end{enumerate}
% Here, $||.||_2$ represents the L$_2$ norm, $\odot$ denotes element-wise multiplication, and $\cdot_{\mathbf{\overrightarrow{i}}}$ represents an indexing operation that extracts slices at indices specified by $\mathbf{\overrightarrow{i}}$. 
\subsubsection{Max pooling} Max pooling selects the node with the maximum feature value from each neighborhood and aggregates their information to create a coarser representation of the graph. It emphasizes the most salient nodes in each neighborhood, capturing important features while reducing redundancy.
% \subsubsection{Self-attention graph pooling} This method leverages attention mechanisms to dynamically weight the contributions of neighboring nodes based on their importance and , allowing nodes to attend to relevant information in their neighborhoods, facilitating adaptive aggregation and effective summarization of the graph.
% Graph pooling aggregates node features to produce a compact representation of the entire graph, which can be fed into subsequent layers or used for downstream tasks. \\
\subsection{Graph unpooling}
Graph unpooling is a complementary operation to graph pooling, aimed at upsampling or reconstructing the original graph representation after downsampling. While graph pooling creates a coarser representation, graph unpooling aims to recover the finer details and restore the original graph structure. Some common types of graph unpooling include,
\subsubsection{Nearest neighbor interpolation} 
% This method aims to recover the original graph topology by identifying the nearest neighbors of pooled nodes in the coarse representation and reinstating unpooled nodes based on their proximity. It reconstructs edges between unpooled nodes and their nearest neighbors, restoring connectivity and preserving local structure.
%  Nearest neighbor unpooling is effective in capturing local relationships and structural patterns in the graph. 
\gls{k-NN} interpolation, introduced with PointNet++ \cite{pnpp} for graphs, is a technique used in GNNs to upsample or unpool graph data. In this method, for each node in the coarser graph, the k nearest neighbors from the finer graph are identified. Then, the features of these k-NN nodes are combined to interpolate the features of the target node. Let \(z\) be a node from the coarser mesh \(M_1\), and assume its \(k\) nearest neighbors on the finer mesh \(M_2\) are denoted as \(x_1, \ldots, x_k\). For a node feature \(f\), the interpolated feature \(f(z)\) of \(M_1\) is defined based on the features of the $k$ nearest points to $z$ on \(M_2\) as,
\begin{equation}
  \mathbf{f}(\mathbf{z})=\frac{\sum_{i=1}^k w\left(x_i\right) \mathbf{f}\left(x_i\right)}{\sum_{i=1}^k w\left(x_i\right)} \text {, where } w\left(x_i\right)=\frac{1}{\left\|z-x_i\right\|_2}
  \end{equation}
\subsubsection{Max unpooling} A common unpooling strategy used in conjunction with max pooling, the locations of the maximum activations are stored. In max unpooling, these locations serve as masks to place the pooled values back into their original positions in the unpooled feature map.
% \subsection{Message Passing}
% \subsection{Graph Convolutions}
% \subsection{Graph Pooling and Unpooling}
